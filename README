# OpenAlex - Data Processing to build a network
![image](https://github.com/user-attachments/assets/714aacc3-432a-43f6-a8be-21274b9dfa0b)


## Summary of the project:
By working with a snapshot of the complete data of the works available on Open Alex, we intend to extract key information and process this data to build a citation network. 

Information we are currently using: **OpenAlex ID, DOI, publication year, references, and author count**.

## Files
###  `dowloader_aws.py`
  
  This script downloads the "works" data files from an AWS S3 bucket containing the OpenAlex dataset. The files are downloaded to a local directory with the same structure as in the S3 bucket.
#### Usage

1. Run the script:
    ```bash
    python3 dowloader_aws.py
    ```
### `main.py`

This script processes JSON data files from the OpenAlex dataset, extracts relevant information, and saves the processed data into Parquet files. It handles multiple subdirectories and files, ensuring that all data is processed and stored. The script also logs any errors encountered during the process.

#### How It Works

1. **Directory Setup:**
   - Input directory: `data/works/`
   - Output directory: `output/`
   - Log file: `output/processing_log.txt`

2. **Data Extraction:**
   - Reads `.gz` files containing JSON lines.
   - Extracts information such as OpenAlex ID, DOI, publication year, references, and author count from each JSON line.
   - Verifies the integrity of the extracted data by comparing the number of lines read to the number of records extracted.

3. **Data Storage:**
   - Saves the extracted data into Parquet files.
   - Each subdirectory in the input directory has a corresponding subdirectory in the output directory.
   - Two types of Parquet files are created for each input file: 
     - `*_main.parquet` containing the main extracted data.
     - `*_references.parquet` containing the references data.

4. **Logging:**
   - Logs errors encountered during processing in a log file.
   - If no errors are encountered, the log file indicates successful completion.

#### Usage

1. Run the script:
    ```bash
    python3 main.py
    ```

### `parquet_processing.py`

This script processes Parquet files generated by the `main.py` script, combines them into manageable chunks, and saves them into new Parquet files. It handles large datasets by processing and saving data incrementally to avoid excessive memory usage.

#### How It Works

1. **Directory Setup:**
   - Input directory: `../output/`
   - Output directory: `../final_files/`

2. **Chunk Processing:**
   - Initializes lists to temporarily store edges and works data.
   - Iterates through the input directory to find and process `_references.parquet` and `_main.parquet` files.
   - Extracts citation network edges and works data, storing them in temporary lists.
   - Saves data in chunks to avoid excessive memory usage. Chunks are saved when the combined size exceeds a specified limit (`chunk_size`).

3. **Data Storage:**
   - Saves each chunk of data to a new Parquet file in the `final_files` directory.
   - Ensures that the directory structure is maintained, and files are named appropriately to indicate their chunk number.

#### Usage

1. Ensure the output files from `main.py` are in the `output/` directory.
2. Run the script:
    ```bash
    python parquet_processing.py
    ```




