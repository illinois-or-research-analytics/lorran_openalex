# OpenAlex: constructing a citation graph from open data.
![image](https://github.com/user-attachments/assets/714aacc3-432a-43f6-a8be-21274b9dfa0b)

# Introduction
This project consists of constructing a citation graph from open data.
Specifically, by parsing the Works section of the [Open Alex](https://arxiv.org/abs/2205.01833) catalog of the global research system. Open Alex contains detailed information about scholarly research, including articles, authors, journals, institutions, and their relationships.
# Motivation
• Open Alex is larger and more comprehensive than other open sources.

• The Works network (scientific documents) can be enriched with metadata, e.g. authors and funding.

• The network can be enriched by querying other open and proprietary APIs, e.g. Crossref and Scopus.

• This large network (163 million nodes) will be used for benchmarking clustering algorithms.
# Summary Features

• Total Nodes (Documents): 256,997,006

• Total Edges (citations): 2,148,871,058

• Documents with DOIs : 163,495,446

• Edges between documents with DOIs : 1,936,722,541

# Content in the Illinois Data Bank

Two `.xz` files, which contain one table each:

**openalexID_newID_hasDOI.parquet.xz**

This table contains three columns: `openalex_id`, `integer_id`, and `hasDOI`. Each row represents a record with the following data types:

- `openalex_id`: A string representing the unique identifier.
- `integer_id`: An integer representing the new identifier.
- `hasDOI`: An integer (0 or 1) indicating whether the record has a DOI (0 for no, 1 for yes).

| openalex_id | integer_id | hasDOI |
|-------------|--------|--------|
| W100000002  | 0      | 1      |
| ...  | ...      | ...      |

**citation_table.tsv.xz**

This table do not have a header. 
The columns represent ***citing*** and ***cited*** integer_id, respectively. 
  
|           |            |
|-----------|------------|
| 211259820 | 135967617  |
| 57594277  | 5687688    |
| 6069762   | 12271761   |
| ...       | ...        |


# In this project: 

Information we are currently using: 

**OpenAlex ID, DOI, publication year, references, and author count**.

## Code Files
### Requirements: 

  - `requirements.txt`
###  `dowloader_aws.py`
  
  This script downloads the "works" data files from an AWS S3 bucket containing the OpenAlex dataset. The files are downloaded to a local directory (openalex-snapshot) with the same structure as in the S3 bucket.

### `main.py`

This script processes the works downloaded into the local directory (openalex-snapshot), extracts relevant information, and saves the processed data into Parquet files. The output directory is 'output'. 

#### How It Works

1. **Directory Setup:**
   - Input directory: `openalex-snapshot/data/works/`
   - Output directory: `output/`
   - Log file: `output/processing_log.txt`

2. **Data Extraction:**
   - Reads `.gz` files containing JSON lines.
   - Extracts information such as OpenAlex ID, DOI, publication year, references, and author count from each JSON line.
   - Verifies the integrity of the extracted data by comparing the number of lines read to the number of records extracted.

3. **Data Storage:**
   - Saves the extracted data into Parquet files.
   - Each subdirectory in the input directory has a corresponding subdirectory in the output directory.
   - Two types of Parquet files are created for each input file: 
     - `*_main.parquet` containing the main extracted data (OpenAlex ID, DOI, publication year and author count)
     - `*_references.parquet` containing the references data.

4. **Logging:**
   - Logs errors encountered during processing in a log file.
   - If no errors are encountered, the log file indicates successful completion.


### `parquet_processing.py`

This script processes Parquet files generated by the `main.py` script, combines them into manageable chunks, and saves them into new Parquet files. It handles large datasets by processing and saving data incrementally to avoid excessive memory usage.

#### How It Works

1. **Directory Setup:**
   - Input directory: `../output/`
   - Output directory: `../preprocessed_files/`

2. **Chunk Processing:**
   - Initializes lists to temporarily store edges and works data.
   - Iterates through the input directory to find and process `_references.parquet` and `_main.parquet` files.
   - Extracts citation network edges and works data, storing them in temporary lists.
   - Saves data in chunks to avoid excessive memory usage. Chunks are saved when the combined size exceeds a specified limit (`chunk_size`).

### `gather_works.py`

  This script reads the `_main.parquet` files created from `parquet_processing.py` to create a single table **'works.parquet'**. This table has: OpenAlex ID, DOI, publication year and author count. 
- Output directory: `../final_files/`
  
| openalex_id | DOI                      | publication_year | author_count | new_id |
|-------------|--------------------------|------------------|--------------|--------|
| W100000002  | 10.1007/978-1-4684-5568-7_3 | 1988             | 2            | 0      |

### `create_id_table.py`

  This script creates the table **"openalexID_integer_id_hasDOI.parquet"** based on "works.parquet" (gather_works.py). This table has 3 informations for each work: openalex_id, has_DOI (0 or 1), and a new integer ID named newID.
- Output directory: `../final_files/`

| openalex_id | integer_id | hasDOI |
|-------------|--------|--------|
| W100000002  | 0      | 1      |


### `create_citation_table.py`

  This script reads the `_references.parquet` files created by `parquet_processing.py` and also the "openalexID_integer_id_hasDOI.parquet" table to create the citation table. Following the same strategy used before, it saves the citation table into manageable chunks. 
- Output directory: `../citations/`
- Multiple tables like the one bellow:

| citing    | citing_hasDOI | cited      | cited_hasDOI |
|-----------|---------------|------------|--------------|
| 211259820 | 1             | 135967617  | 1            |
| 57594277  | 1             | 5687688    | 1            |
| 6069762   | 0             | 12271761   | 1            |


### `concat_citation_table.py`

  This script creates a single citation table from the chunks created before. The output file is **citation_table.parquet**.
- Output directory: `../final_files/`

| citing    | citing_hasDOI | cited      | cited_hasDOI |
|-----------|---------------|------------|--------------|
| 211259820 | 1             | 135967617  | 1            |
| 57594277  | 1             | 5687688    | 1            |
| 6069762   | 0             | 12271761   | 1            |


### `create_citation_tsv.py`
  This script creates the **citation_table.tsv**, available on the databank. It takes the columns *citing* and *cited*  from the previous *citation_table.parquet*. 
- This table do not have a header. The columns represent *citing* and *cited*, respectively. 

|           |            |
|-----------|------------|
| 211259820 | 135967617  |
| 57594277  | 5687688    |
| 6069762   | 12271761   |


### `parquet_reader.py`

  A simple code to read and check the files. 

# Future Projects
• Community detection and visualization.
