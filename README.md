# OpenAlex - Data Processing to build a network
![image](https://github.com/user-attachments/assets/714aacc3-432a-43f6-a8be-21274b9dfa0b)


## Summary of the project:
By working with a snapshot of the complete data of the works available on Open Alex, we intend to extract key information and process this data to build a citation network. 

Information we are currently using: **OpenAlex ID, DOI, publication year, references, and author count**.

## Files
###  `dowloader_aws.py`
  
  This script downloads the "works" data files from an AWS S3 bucket containing the OpenAlex dataset. The files are downloaded to a local directory (openalex-snapshot) with the same structure as in the S3 bucket.

### `main.py`

This script processes the works downloaded into the local directory (openalex-snapshot), extracts relevant information, and saves the processed data into Parquet files. The output directory is 'output'. 

#### How It Works

1. **Directory Setup:**
   - Input directory: `openalex-snapshot/data/works/`
   - Output directory: `output/`
   - Log file: `output/processing_log.txt`

2. **Data Extraction:**
   - Reads `.gz` files containing JSON lines.
   - Extracts information such as OpenAlex ID, DOI, publication year, references, and author count from each JSON line.
   - Verifies the integrity of the extracted data by comparing the number of lines read to the number of records extracted.

3. **Data Storage:**
   - Saves the extracted data into Parquet files.
   - Each subdirectory in the input directory has a corresponding subdirectory in the output directory.
   - Two types of Parquet files are created for each input file: 
     - `*_main.parquet` containing the main extracted data (OpenAlex ID, DOI, publication year and author count)
     - `*_references.parquet` containing the references data.

4. **Logging:**
   - Logs errors encountered during processing in a log file.
   - If no errors are encountered, the log file indicates successful completion.


### `parquet_processing.py`

This script processes Parquet files generated by the `main.py` script, combines them into manageable chunks, and saves them into new Parquet files. It handles large datasets by processing and saving data incrementally to avoid excessive memory usage.

#### How It Works

1. **Directory Setup:**
   - Input directory: `../output/`
   - Output directory: `../preprocessed_files/`

2. **Chunk Processing:**
   - Initializes lists to temporarily store edges and works data.
   - Iterates through the input directory to find and process `_references.parquet` and `_main.parquet` files.
   - Extracts citation network edges and works data, storing them in temporary lists.
   - Saves data in chunks to avoid excessive memory usage. Chunks are saved when the combined size exceeds a specified limit (`chunk_size`).


### `gather_works.py`

  This script reads the `_main.parquet` files created from `parquet_processing.py` to create a single table **'works.parquet'**. This table has: OpenAlex ID, DOI, publication year and author count. 
- Output directory: `../final_files/`

### `create_id_table.py`

  This script creates the table **"openalexID_newID_hasDOI.parquet"** based on "works.parquet" (gather_works.py). This table has 3 informations for each work: openalex_id, has_DOI (0 or 1), and a new integer ID named newID.
- Output directory: `../final_files/`

### `create_citation_table.py`

  This script reads the `_references.parquet` files created by `parquet_processing.py` and also the "openalexID_newID_hasDOI.parquet" table to create the citation table. Following the same strategy used before, it saves the citation table into manageable chunks. 
- Output directory: `../citations/`

### `concat_citation_table.py`

  This script creates a single citation table from the chunks created before. The output file is **combined_citation_table.parquet**.
- Output directory: `../final_files/`

### `parquet_reader.py`

  A simple code to read and check the files. 










